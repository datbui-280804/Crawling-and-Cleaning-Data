import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import random
import re

# C·∫§U H√åNH
JOB_POSITIONS = [
    "Tester",
    "Backend Developer",
    "Frontend Developer",
    "Fullstack Developer",
    "Data Engineer",
    "Data Analyst",
    "Data Scientist",
    "DevOps Engineer",
    "AI Engineer",
    "Mobile Developer"
]

BASE_URL = "https://123job.vn"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def build_search_url(job_name):
    return f"{BASE_URL}/tuyen-dung?q={job_name.replace(' ', '+')}"

def crawl_jobs():
    all_jobs = []

    for position in JOB_POSITIONS:
        print(f"üîç ƒêang crawl v·ªã tr√≠: {position}")
        url = build_search_url(position)

        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(response.text, "html.parser")
            job_items = soup.find_all("div", class_="job__list-item")
            
            # In ra s·ªë l∆∞·ª£ng t√¨m th·∫•y ƒë·ªÉ b·∫°n ki·ªÉm tra
            print(f"   -> T√¨m th·∫•y {len(job_items)} c√¥ng vi·ªác tr√™n trang n√†y.")

            # --- THAY ƒê·ªîI ·ªû ƒê√ÇY: ƒê√£ b·ªè [:10] ƒë·ªÉ ch·∫°y h·∫øt danh s√°ch ---
            for item in job_items: 
                try:
                    # 1. L·∫•y th√¥ng tin c∆° b·∫£n
                    title_tag = item.find("h2", class_="job__list-item-title")
                    if not title_tag: continue
                    a_tag = title_tag.find("a")
                    job_name = a_tag.get_text(strip=True)
                    job_link = a_tag["href"]
                    if not job_link.startswith("http"):
                        job_link = BASE_URL + job_link

                    company_tag = item.find("div", class_="job__list-item-company")
                    company = company_tag.find("span").get_text(strip=True) if company_tag else "N/A"

                    # 2. V√ÄO TRANG CHI TI·∫æT
                    experience = "N/A"
                    salary = "N/A"
                    location = "N/A"
                    full_description = "N/A"
                    
                    # TƒÉng delay m·ªôt ch√∫t ƒë·ªÉ an to√†n khi crawl s·ªë l∆∞·ª£ng l·ªõn h∆°n
                    time.sleep(random.uniform(0.8, 1.5)) 
                    
                    try:
                        res_detail = requests.get(job_link, headers=HEADERS, timeout=10)
                        soup_detail = BeautifulSoup(res_detail.text, "html.parser")

                        # LOGIC QU√âT ATTR-ITEM (L·∫•y L∆∞∆°ng, Kinh nghi·ªám, ƒê·ªãa ƒëi·ªÉm)
                        attr_items = soup_detail.find_all("div", class_="attr-item")
                        
                        for attr in attr_items:
                            full_text = attr.get_text(strip=True).lower()
                            value_div = attr.find("div", class_="value")
                            
                            if value_div:
                                value_text = value_div.get_text(strip=True)
                                if "kinh nghi·ªám" in full_text:
                                    experience = value_text
                                elif "l∆∞∆°ng" in full_text:
                                    salary = value_text
                                elif "ƒë·ªãa ƒëi·ªÉm" in full_text:
                                    location = value_text

                        # Fallback cho Location n·∫øu ch∆∞a t√¨m th·∫•y
                        if location == "N/A":
                            address_div = soup_detail.find("div", class_="job-detail__info-address")
                            if address_div:
                                location = address_div.get_text(strip=True).replace("ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác:", "").strip()

                        # L·∫•y N·ªôi Dung
                        content_div = soup_detail.find("div", class_="content-collapse")
                        if content_div:
                             full_description = content_div.get_text(separator="\n").strip()
                        
                    except Exception as e:
                        print(f"‚ö† L·ªói detail link: {e}")

                    all_jobs.append({
                        "position_search": position,
                        "job_name": job_name,
                        "job_link": job_link,
                        "company": company,
                        "location": location,
                        "salary": salary,
                        "experience": experience,
                        "job_description": full_description,
                        "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })

                except Exception as e:
                    print(f"‚ö† L·ªói item: {e}")
                    continue

        except Exception as e:
            print(f"‚ùå L·ªói m·∫°ng: {url}")

    return pd.DataFrame(all_jobs)

if __name__ == "__main__":
    os.makedirs("data", exist_ok=True)
    df = crawl_jobs()
    print(f"\n‚úÖ T·ªïng s·ªë job crawl ƒë∆∞·ª£c: {len(df)}")
    df.to_csv("data/raw_jobs.csv", index=False, encoding="utf-8-sig")